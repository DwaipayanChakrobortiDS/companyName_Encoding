{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully converted to CSV.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Load the joblib file\n",
    "data = joblib.load(r'C:\\Users\\Dwaipayan C\\Downloads\\Monthly_Income_Estimation_Target_Encoded_Artifacts_companytrgencode.joblib')\n",
    "\n",
    "# Ensure that the loaded data is a DataFrame\n",
    "if isinstance(data, pd.DataFrame):\n",
    "    # Save the DataFrame to a CSV file\n",
    "    data.to_csv('output_file_name.csv', index=False)\n",
    "    print(\"Data has been successfully converted to CSV.\")\n",
    "else:\n",
    "    print(\"Loaded data is not a DataFrame. Please ensure your joblib file contains a DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully converted to CSV and compressed into a ZIP file.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "# Load the joblib file\n",
    "data = joblib.load(r'C:\\Users\\Dwaipayan C\\Downloads\\Monthly_Income_Estimation_Target_Encoded_Artifacts_companytrgencode.joblib')\n",
    "\n",
    "# Ensure that the loaded data is a DataFrame\n",
    "if isinstance(data, pd.DataFrame):\n",
    "    # Save the DataFrame to a CSV file\n",
    "    csv_file = 'output_file_name.csv'\n",
    "    data.to_csv(csv_file, index=False)\n",
    "    \n",
    "    # Compress the CSV file into a ZIP file\n",
    "    zip_file = 'output_file_name.zip'\n",
    "    with zipfile.ZipFile(zip_file, 'w', zipfile.ZIP_DEFLATED) as z:\n",
    "        z.write(csv_file)\n",
    "    \n",
    "    print(\"Data has been successfully converted to CSV and compressed into a ZIP file.\")\n",
    "else:\n",
    "    print(\"Loaded data is not a DataFrame. Please ensure your joblib file contains a DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    global_mean  smoothing_factor       loan_company_name  \\\n",
      "0  30892.506245                15                     NaN   \n",
      "1  30892.506245                15  Expert Security Agency   \n",
      "2  30892.506245                15  leekie enterprises inc   \n",
      "3  30892.506245                15          Mangiboa Store   \n",
      "4  30892.506245                15           Fruits Vendor   \n",
      "\n",
      "   freq_encodedcompanyName  category_mean_companyName  \\\n",
      "0                      NaN                        NaN   \n",
      "1                      1.0               25000.000000   \n",
      "2                      1.0               28000.000000   \n",
      "3                      1.0               15000.000000   \n",
      "4                      3.0               28333.333333   \n",
      "\n",
      "   category_size_companyName  target_encodedcompanyName  \\\n",
      "0                        NaN               30892.506245   \n",
      "1                        1.0               30524.224605   \n",
      "2                        1.0               30711.724605   \n",
      "3                        1.0               29899.224605   \n",
      "4                        3.0               30465.977426   \n",
      "\n",
      "   encoded_company_name_group  \n",
      "0                         NaN  \n",
      "1                30524.224605  \n",
      "2                30711.724605  \n",
      "3                29899.224605  \n",
      "4                91397.932279  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(r'C:\\Users\\Dwaipayan C\\OneDrive\\Learning\\Projects\\Income_Estimation\\output_file_name.csv')\n",
    "\n",
    "# Preview the data\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64771"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['loan_company_name'].drop_duplicates().unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        loan_company_name           clean_name_re\n",
      "0                     NaN                     nan\n",
      "1  Expert Security Agency  expert security agency\n",
      "2  leekie enterprises inc      leekie enterprises\n",
      "3          Mangiboa Store          mangiboa store\n",
      "4           Fruits Vendor           fruits vendor\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean company names\n",
    "def preprocess_name(name):\n",
    "    name = str(name).lower()  # Convert to lowercase\n",
    "    name = re.sub(r'[^\\w\\s]', '', name)  # Remove punctuation\n",
    "    # Remove common suffixes\n",
    "    suffixes = ['inc', 'incorporated', 'corp', 'corporation', 'ltd', 'limited', 'co', 'company', 'plc']\n",
    "    pattern = r'\\b(?:{})\\b'.format('|'.join(suffixes))\n",
    "    name = re.sub(pattern, '', name)\n",
    "    name = ' '.join(name.split())  # Remove extra spaces\n",
    "    return name\n",
    "\n",
    "# Apply preprocessing\n",
    "df['clean_name_re'] = df['loan_company_name'].apply(preprocess_name)\n",
    "\n",
    "# Preview cleaned names\n",
    "print(df[['loan_company_name', 'clean_name_re']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step3 - Using rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company names have been standardized and saved to 'rapidfuzzname_companyName.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "# Function to clean company names\n",
    "def preprocess_name(name):\n",
    "    name = str(name).lower()  # Convert to lowercase\n",
    "    name = re.sub(r'[^\\w\\s]', '', name)  # Remove punctuation\n",
    "    # Remove common suffixes\n",
    "    suffixes = ['inc', 'incorporated', 'corp', 'corporation', 'ltd', 'limited', 'co', 'company', 'plc']\n",
    "    pattern = r'\\b(?:{})\\b'.format('|'.join(suffixes))\n",
    "    name = re.sub(pattern, '', name)\n",
    "    name = ' '.join(name.split())  # Remove extra spaces\n",
    "    return name\n",
    "\n",
    "# Load your data\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\Dwaipayan C\\OneDrive\\Learning\\Projects\\Income_Estimation\\output_file_name.csv')\n",
    "# Apply preprocessing\n",
    "df['clean_name_rapidfuzz'] = df['loan_company_name'].apply(preprocess_name)\n",
    "\n",
    "# Get a list of unique cleaned names\n",
    "unique_names = df['clean_name_rapidfuzz'].unique()\n",
    "\n",
    "# Create a dictionary to map similar names\n",
    "name_mapping = {}\n",
    "\n",
    "for name in unique_names:\n",
    "    if name in name_mapping.values():\n",
    "        continue  # Skip if already mapped\n",
    "    # Find matches with a similarity score above the threshold\n",
    "    matches = process.extract(\n",
    "        name,\n",
    "        unique_names,\n",
    "        scorer=fuzz.token_sort_ratio,  # Use fuzz.token_sort_ratio here\n",
    "        score_cutoff=90  # Adjust this threshold as needed\n",
    "    )\n",
    "    # Map similar names to the representative name\n",
    "    for match_name, score, _ in matches:\n",
    "        name_mapping[match_name] = name\n",
    "\n",
    "# Map the standardized names back to the DataFrame\n",
    "df['standardized_name'] = df['clean_name_rapidfuzz'].map(name_mapping)\n",
    "\n",
    "# Optionally, format the standardized names\n",
    "df['rapidfuzzname'] = df['standardized_name'].str.title()\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "df.to_csv('rapidfuzzname_companyName.csv', index=False)\n",
    "\n",
    "print(\"Company names have been standardized and saved to 'rapidfuzzname_companyName.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using jellyfish for Phonetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        loan_company_name standardized_name_jellyfish\n",
      "0                     NaN                       Noona\n",
      "1  Expert Security Agency      Expert Security Agency\n",
      "2  leekie enterprises inc      leekie enterprises inc\n",
      "3          Mangiboa Store              Mangiboa Store\n",
      "4           Fruits Vendor               Fruits Vendor\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jellyfish\n",
    "\n",
    "# Function to get phonetic code\n",
    "def get_phonetic_code(name):\n",
    "    name = str(name)\n",
    "    code = jellyfish.metaphone(name)\n",
    "    return code\n",
    "\n",
    "# Apply phonetic coding\n",
    "df['phonetic_code'] = df['loan_company_name'].apply(get_phonetic_code)\n",
    "\n",
    "# Map standardized names based on phonetic codes\n",
    "df['standardized_name_jellyfish'] = df.groupby('phonetic_code')['loan_company_name'].transform('first')\n",
    "\n",
    "# Preview the results\n",
    "print(df[['loan_company_name', 'standardized_name_jellyfish']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data to a new csv file\n",
    "\n",
    "df.to_csv(\"Phonetic_cleaned_company_names.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Untilizing Hashing of n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        loan_company_name standardized_name_ngram_hash\n",
      "0                     NaN               Mangiboa Store\n",
      "1  Expert Security Agency       Expert Security Agency\n",
      "2  leekie enterprises inc       leekie enterprises inc\n",
      "3          Mangiboa Store               Mangiboa Store\n",
      "4           Fruits Vendor               Mangiboa Store\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import hashlib\n",
    "\n",
    "# Function to generate n-gram hash\n",
    "def ngram_hash(name, n=3):\n",
    "    name = str(name).lower()\n",
    "    name = re.sub(r'[^\\w\\s]', '', name)\n",
    "    tokens = name.split()\n",
    "    ngrams = [''.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "    combined = ' '.join(ngrams)\n",
    "    return hashlib.md5(combined.encode()).hexdigest()\n",
    "\n",
    "# Apply n-gram hashing\n",
    "df['ngram_hash'] = df['loan_company_name'].apply(ngram_hash)\n",
    "\n",
    "# Map standardized names\n",
    "df['standardized_name_ngram_hash'] = df.groupby('ngram_hash')['loan_company_name'].transform('first')\n",
    "\n",
    "# Preview the results\n",
    "print(df[['loan_company_name', 'standardized_name_ngram_hash']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Ngram_cleaned_company_names.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Exact Matching with Extensive Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        loan_company_name standardized_name_aggressive_normalize\n",
      "0                     NaN                                    nan\n",
      "1  Expert Security Agency                 agency expert security\n",
      "2  leekie enterprises inc                     enterprises leekie\n",
      "3          Mangiboa Store                         mangiboa store\n",
      "4           Fruits Vendor                          fruits vendor\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "# Function to perform aggressive normalization\n",
    "def aggressive_normalize(name):\n",
    "    name = str(name).lower()\n",
    "    name = unicodedata.normalize('NFKD', name).encode('ascii', 'ignore').decode('utf-8')\n",
    "    name = re.sub(r'[^\\w\\s]', '', name)\n",
    "    name = re.sub(r'\\d+', '', name)  # Remove digits\n",
    "    stop_words = ['the', 'and', 'of', 'in', 'a']\n",
    "    tokens = [word for word in name.split() if word not in stop_words]\n",
    "    suffixes = ['inc', 'incorporated', 'corp', 'corporation', 'company', 'co', 'ltd', 'limited', 'plc']\n",
    "    tokens = [word for word in tokens if word not in suffixes]\n",
    "    name = ' '.join(sorted(tokens))\n",
    "    return name.strip()\n",
    "\n",
    "# Apply aggressive normalization\n",
    "df['standardized_name_aggressive_normalize'] = df['loan_company_name'].apply(aggressive_normalize)\n",
    "\n",
    "# Preview the results\n",
    "print(df[['loan_company_name', 'standardized_name_aggressive_normalize']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Aggressive_cleaned_company_names.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Clustering with String Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        loan_company_name standardized_name_dbscan\n",
      "0                     NaN                     None\n",
      "1  Expert Security Agency   Expert Security Agency\n",
      "2  leekie enterprises inc   Expert Security Agency\n",
      "3          Mangiboa Store   Expert Security Agency\n",
      "4           Fruits Vendor   Expert Security Agency\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Preprocess names\n",
    "def preprocess(name):\n",
    "    name = str(name).lower()\n",
    "    name = re.sub(r'[^\\w\\s]', '', name)\n",
    "    return name\n",
    "\n",
    "df['clean_name_dbscan'] = df['loan_company_name'].apply(preprocess)\n",
    "\n",
    "# Vectorize names\n",
    "vectorizer = TfidfVectorizer(min_df=1, analyzer='char', ngram_range=(2,3))\n",
    "tfidf_matrix = vectorizer.fit_transform(df['clean_name_dbscan'])\n",
    "\n",
    "# Cluster names using DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=2, metric='cosine')\n",
    "dbscan.fit(tfidf_matrix)\n",
    "\n",
    "# Assign cluster labels\n",
    "df['cluster'] = dbscan.labels_\n",
    "\n",
    "# Map standardized names\n",
    "df['standardized_name_dbscan'] = df.groupby('cluster')['loan_company_name'].transform('first')\n",
    "\n",
    "# Preview the results\n",
    "print(df[['loan_company_name', 'standardized_name_dbscan']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"DBSCAN_cleaned_company_names.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Recommendation**\n",
    "\n",
    "Given your need for speed and efficiency, I suggest starting with extensive normalization (Option 1) and only moving to clustering methods if necessary. Aggressive preprocessing often resolves most of the standardization issues without heavy computational costs.\n",
    "\n",
    "**Additional Thoughts**\n",
    "\n",
    "`Manual Inspection:` After applying automated methods, it's helpful to review a sample to ensure accuracy.\n",
    "\n",
    "`Hybrid Approach:` Combine methods for better results (e.g., normalization followed by phonetic coding).\n",
    "\n",
    "`Data Quality Feedback Loop:` As you identify recurring issues, update your preprocessing functions to handle them.\n",
    "\n",
    "**Sample Workflow**\n",
    "\n",
    "`Normalize Names Aggressively:` Clean and standardize as much as possible.\n",
    "\n",
    "`Apply Phonetic Encoding (Optional):` If needed, to catch phonetically similar names.\n",
    "\n",
    "`Cluster Remaining Variations:` Use clustering for names that are still not matching.\n",
    "\n",
    "`Validate Results:` Check samples to ensure that unrelated companies aren't merged.\n",
    "\n",
    "`Iterate and Refine:` Adjust parameters and methods based on findings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claude code for pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  loan_company_name  \\\n",
      "0                                               NaN   \n",
      "1                            Expert Security Agency   \n",
      "2                            leekie enterprises inc   \n",
      "3                                    Mangiboa Store   \n",
      "4                                     Fruits Vendor   \n",
      "...                                             ...   \n",
      "64769                                           MCW   \n",
      "64770                                      LGU Bago   \n",
      "64771                               Lending Company   \n",
      "64772                                           HRG   \n",
      "64773  sanmiguel integrated logistics services inc.   \n",
      "\n",
      "                              clean_name_re  \\\n",
      "0                                       nan   \n",
      "1                      expertsecurityagency   \n",
      "2                         leekieenterprises   \n",
      "3                             mangiboastore   \n",
      "4                              fruitsvendor   \n",
      "...                                     ...   \n",
      "64769                                   mcw   \n",
      "64770                               lgubago   \n",
      "64771                               lending   \n",
      "64772                                   hrg   \n",
      "64773  sanmiguelintegratedlogisticsservices   \n",
      "\n",
      "                standardized_name_jellyfish  \\\n",
      "0                                       nan   \n",
      "1                      expertsecurityagency   \n",
      "2                         leekieenterprises   \n",
      "3                             mangiboastore   \n",
      "4                              fruitsvendor   \n",
      "...                                     ...   \n",
      "64769                                   mec   \n",
      "64770                               lgubago   \n",
      "64771                               lending   \n",
      "64772                                   hrg   \n",
      "64773  sanmiguelintegratedlogisticsservices   \n",
      "\n",
      "               standardized_name_ngram_hash  \\\n",
      "0                                       nan   \n",
      "1                      expertsecurityagency   \n",
      "2                         leekieenterprises   \n",
      "3                             mangiboastore   \n",
      "4                              fruitsvendor   \n",
      "...                                     ...   \n",
      "64769                                   mec   \n",
      "64770                               lgubago   \n",
      "64771                               lending   \n",
      "64772                                   hrg   \n",
      "64773  sanmiguelintegratedlogisticsservices   \n",
      "\n",
      "      standardized_name_aggressive_normalize  \n",
      "0                                        nan  \n",
      "1                       expertsecurityagency  \n",
      "2                          leekieenterprises  \n",
      "3                              mangiboastore  \n",
      "4                               fruitsvendor  \n",
      "...                                      ...  \n",
      "64769                                    mec  \n",
      "64770                                lgubago  \n",
      "64771                                lending  \n",
      "64772                                    hrg  \n",
      "64773   sanmiguelintegratedlogisticsservices  \n",
      "\n",
      "[64774 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import jellyfish\n",
    "import hashlib\n",
    "import unicodedata\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class CompanyNameStandardizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, input_col='loan_company_name'):\n",
    "        self.input_col = input_col\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Create a copy to avoid modifying the original\n",
    "        df = X.copy()\n",
    "        \n",
    "        # Step 1: Basic preprocessing\n",
    "        df['clean_name_re'] = df[self.input_col].apply(self._preprocess_name)\n",
    "        \n",
    "        # Step 2: Phonetic coding\n",
    "        df['phonetic_code'] = df['clean_name_re'].apply(self._get_phonetic_code)\n",
    "        df['standardized_name_jellyfish'] = df.groupby('phonetic_code')['clean_name_re'].transform('first')\n",
    "        \n",
    "        # Step 3: N-gram hashing\n",
    "        df['ngram_hash'] = df['standardized_name_jellyfish'].apply(self._ngram_hash)\n",
    "        df['standardized_name_ngram_hash'] = df.groupby('ngram_hash')['standardized_name_jellyfish'].transform('first')\n",
    "        \n",
    "        # Step 4: Aggressive normalization\n",
    "        df['standardized_name_aggressive_normalize'] = df['standardized_name_ngram_hash'].apply(self._aggressive_normalize)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _preprocess_name(self, name):\n",
    "        name = str(name).lower()  # Convert to lowercase\n",
    "        name = re.sub(r'[^\\w\\s]', '', name)  # Remove punctuation\n",
    "        \n",
    "        # Remove common suffixes\n",
    "        suffixes = ['inc', 'incorporated', 'corp', 'corporation', 'ltd', 'limited', 'co', 'company', 'plc']\n",
    "        pattern = r'\\b(?:{})\\b'.format('|'.join(suffixes))\n",
    "        name = re.sub(pattern, '', name)\n",
    "        \n",
    "        name = ' '.join(name.split())  # Remove extra spaces\n",
    "        name = name.replace(' ', '')  # Remove spaces between words\n",
    "        \n",
    "        return name\n",
    "    \n",
    "    def _get_phonetic_code(self, name):\n",
    "        name = str(name)\n",
    "        code = jellyfish.metaphone(name)\n",
    "        return code\n",
    "    \n",
    "    def _ngram_hash(self, name, n=3):\n",
    "        name = str(name).lower()\n",
    "        name = re.sub(r'[^\\w\\s]', '', name)\n",
    "        tokens = name.split()\n",
    "        ngrams = [''.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)] if len(tokens) >= n else [name]\n",
    "        combined = ' '.join(ngrams)\n",
    "        return hashlib.md5(combined.encode()).hexdigest()\n",
    "    \n",
    "    def _aggressive_normalize(self, name):\n",
    "        name = str(name).lower()\n",
    "        name = unicodedata.normalize('NFKD', name).encode('ascii', 'ignore').decode('utf-8')\n",
    "        name = re.sub(r'[^\\w\\s]', '', name)\n",
    "        name = re.sub(r'\\d+', '', name)  # Remove digits\n",
    "        \n",
    "        stop_words = ['the', 'and', 'of', 'in', 'a']\n",
    "        tokens = [word for word in name.split() if word not in stop_words]\n",
    "        \n",
    "        suffixes = ['inc', 'incorporated', 'corp', 'corporation', 'company', 'co', 'ltd', 'limited', 'plc']\n",
    "        tokens = [word for word in tokens if word not in suffixes]\n",
    "        \n",
    "        name = ' '.join(sorted(tokens))\n",
    "        return name.strip().lower()  # Ensure final result is lowercase\n",
    "\n",
    "# Create the pipeline\n",
    "def create_company_name_pipeline(input_col='loan_company_name'):\n",
    "    pipeline = Pipeline([\n",
    "        ('company_name_standardizer', CompanyNameStandardizer(input_col=input_col))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data\n",
    "\n",
    "    \n",
    "    # Apply the pipeline\n",
    "    pipeline = create_company_name_pipeline()\n",
    "    result_df = pipeline.transform(df)\n",
    "    \n",
    "    # Display results\n",
    "    print(result_df[[\n",
    "        'loan_company_name', \n",
    "        'clean_name_re',\n",
    "        'standardized_name_jellyfish',\n",
    "        'standardized_name_ngram_hash',\n",
    "        'standardized_name_aggressive_normalize'\n",
    "    ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"Final_cleaned_company_names.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import jellyfish\n",
    "import hashlib\n",
    "import unicodedata\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class CompanyNameCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, ngram_n=3):\n",
    "        # You can add parameters here if needed\n",
    "        self.ngram_n = ngram_n\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # This transformer doesn't learn from the data\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Step 1: Preprocess the names\n",
    "        X['clean_name'] = X['loan_company_name'].apply(self.preprocess_name)\n",
    "        \n",
    "        # Step 2: Apply phonetic coding\n",
    "        X['phonetic_code'] = X['clean_name'].apply(self.get_phonetic_code)\n",
    "        X['standardized_name_phonetic'] = X.groupby('phonetic_code')['loan_company_name'].transform('first')\n",
    "        \n",
    "        # Step 3: Apply n-gram hashing\n",
    "        X['ngram_hash'] = X['standardized_name_phonetic'].apply(self.ngram_hash)\n",
    "        X['standardized_name_ngram'] = X.groupby('ngram_hash')['standardized_name_phonetic'].transform('first')\n",
    "        \n",
    "        # Step 4: Aggressive normalization\n",
    "        X['standardized_name_aggressive'] = X['standardized_name_ngram'].apply(self.aggressive_normalize)\n",
    "        \n",
    "        # Return the DataFrame with the final cleaned column\n",
    "        return X\n",
    "\n",
    "    def preprocess_name(self, name):\n",
    "        if pd.isnull(name):\n",
    "            return ''\n",
    "        name = str(name).lower()\n",
    "        name = re.sub(r'[^\\w\\s]', '', name)       # Remove punctuation\n",
    "        # Remove common suffixes\n",
    "        suffixes = ['inc', 'incorporated', 'corp', 'corporation', 'ltd', 'limited', 'co', 'company', 'plc']\n",
    "        pattern = r'\\b(?:{})\\b'.format('|'.join(suffixes))\n",
    "        name = re.sub(pattern, '', name)\n",
    "        name = ''.join(name.split())               # Remove all spaces between names\n",
    "        return name\n",
    "\n",
    "    def get_phonetic_code(self, name):\n",
    "        return jellyfish.metaphone(name)\n",
    "\n",
    "    def ngram_hash(self, name, n=3):\n",
    "        name = str(name).lower()\n",
    "        name = re.sub(r'[^\\w\\s]', '', name)\n",
    "        tokens = name.split()\n",
    "        if len(tokens) < n:\n",
    "            ngrams = [''.join(tokens)]\n",
    "        else:\n",
    "            ngrams = [''.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "        combined = ' '.join(ngrams)\n",
    "        return hashlib.md5(combined.encode()).hexdigest()\n",
    "\n",
    "    def aggressive_normalize(self, name):\n",
    "        name = str(name).lower()\n",
    "        # Remove accents\n",
    "        name = unicodedata.normalize('NFKD', name).encode('ascii', 'ignore').decode('utf-8')\n",
    "        name = re.sub(r'[^\\w\\s]', '', name)        # Remove punctuation\n",
    "        name = re.sub(r'\\d+', '', name)            # Remove digits\n",
    "        # Remove stop words\n",
    "        stop_words = ['the', 'and', 'of', 'in', 'a']\n",
    "        tokens = [word for word in name.split() if word not in stop_words]\n",
    "        # Remove suffixes\n",
    "        suffixes = ['inc', 'incorporated', 'corp', 'corporation', 'company', 'co', 'ltd', 'limited', 'plc']\n",
    "        tokens = [word for word in tokens if word not in suffixes]\n",
    "        name = ' '.join(sorted(tokens))            # Sort tokens\n",
    "        return name.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('company_name_cleaner', CompanyNameCleaner(ngram_n=3))\n",
    "])\n",
    "\n",
    "# Assume df is your DataFrame with 'loan_company_name' column\n",
    "# Apply the pipeline\n",
    "df_cleaned = pipeline.transform(df)\n",
    "\n",
    "# View the final standardized names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# # Assuming you have a target variable 'y'\n",
    "# y = [...]  # Your target variable\n",
    "\n",
    "# # Create a full pipeline\n",
    "# full_pipeline = Pipeline([\n",
    "#     ('company_name_cleaner', CompanyNameCleaner()),\n",
    "#     ('vectorizer', CountVectorizer()),\n",
    "#     ('classifier', RandomForestClassifier())\n",
    "# ])\n",
    "\n",
    "# # Fit the pipeline\n",
    "# full_pipeline.fit(df[['loan_company_name']], y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>global_mean</th>\n",
       "      <th>smoothing_factor</th>\n",
       "      <th>loan_company_name</th>\n",
       "      <th>freq_encodedcompanyName</th>\n",
       "      <th>category_mean_companyName</th>\n",
       "      <th>category_size_companyName</th>\n",
       "      <th>target_encodedcompanyName</th>\n",
       "      <th>encoded_company_name_group</th>\n",
       "      <th>clean_name</th>\n",
       "      <th>phonetic_code</th>\n",
       "      <th>standardized_name_phonetic</th>\n",
       "      <th>ngram_hash</th>\n",
       "      <th>standardized_name_ngram</th>\n",
       "      <th>standardized_name_aggressive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30892.506245</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30892.506245</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>EEI Corporation</td>\n",
       "      <td>89aa7553a2ef81ec9076d21e44b4b0d9</td>\n",
       "      <td>EEI Corporation</td>\n",
       "      <td>eei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30892.506245</td>\n",
       "      <td>15</td>\n",
       "      <td>Expert Security Agency</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25000.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30524.224605</td>\n",
       "      <td>30524.224605</td>\n",
       "      <td>expertsecurityagency</td>\n",
       "      <td>EKSPRTSKRTYJNS</td>\n",
       "      <td>Expert Security Agency</td>\n",
       "      <td>4c004b846168151b991c9d36b84d42c8</td>\n",
       "      <td>Expert Security Agency</td>\n",
       "      <td>agency expert security</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30892.506245</td>\n",
       "      <td>15</td>\n",
       "      <td>leekie enterprises inc</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28000.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30711.724605</td>\n",
       "      <td>30711.724605</td>\n",
       "      <td>leekieenterprises</td>\n",
       "      <td>LKNTRPRSS</td>\n",
       "      <td>leekie enterprises inc</td>\n",
       "      <td>b4ab50eb7e380812afbae53f03334c7b</td>\n",
       "      <td>leekie enterprises inc</td>\n",
       "      <td>enterprises leekie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30892.506245</td>\n",
       "      <td>15</td>\n",
       "      <td>Mangiboa Store</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15000.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29899.224605</td>\n",
       "      <td>29899.224605</td>\n",
       "      <td>mangiboastore</td>\n",
       "      <td>MNJBSTR</td>\n",
       "      <td>Mangiboa Store</td>\n",
       "      <td>51250b025291d65f4dac3246c2d961bf</td>\n",
       "      <td>Mangiboa Store</td>\n",
       "      <td>mangiboa store</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30892.506245</td>\n",
       "      <td>15</td>\n",
       "      <td>Fruits Vendor</td>\n",
       "      <td>3.0</td>\n",
       "      <td>28333.333333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30465.977426</td>\n",
       "      <td>91397.932279</td>\n",
       "      <td>fruitsvendor</td>\n",
       "      <td>FRTSFNTR</td>\n",
       "      <td>Fruits Vendor</td>\n",
       "      <td>68a04ab84f0a560ea880f44d1e130008</td>\n",
       "      <td>Fruits Vendor</td>\n",
       "      <td>fruits vendor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    global_mean  smoothing_factor       loan_company_name  \\\n",
       "0  30892.506245                15                     NaN   \n",
       "1  30892.506245                15  Expert Security Agency   \n",
       "2  30892.506245                15  leekie enterprises inc   \n",
       "3  30892.506245                15          Mangiboa Store   \n",
       "4  30892.506245                15           Fruits Vendor   \n",
       "\n",
       "   freq_encodedcompanyName  category_mean_companyName  \\\n",
       "0                      NaN                        NaN   \n",
       "1                      1.0               25000.000000   \n",
       "2                      1.0               28000.000000   \n",
       "3                      1.0               15000.000000   \n",
       "4                      3.0               28333.333333   \n",
       "\n",
       "   category_size_companyName  target_encodedcompanyName  \\\n",
       "0                        NaN               30892.506245   \n",
       "1                        1.0               30524.224605   \n",
       "2                        1.0               30711.724605   \n",
       "3                        1.0               29899.224605   \n",
       "4                        3.0               30465.977426   \n",
       "\n",
       "   encoded_company_name_group            clean_name   phonetic_code  \\\n",
       "0                         NaN                                         \n",
       "1                30524.224605  expertsecurityagency  EKSPRTSKRTYJNS   \n",
       "2                30711.724605     leekieenterprises       LKNTRPRSS   \n",
       "3                29899.224605         mangiboastore         MNJBSTR   \n",
       "4                91397.932279          fruitsvendor        FRTSFNTR   \n",
       "\n",
       "  standardized_name_phonetic                        ngram_hash  \\\n",
       "0            EEI Corporation  89aa7553a2ef81ec9076d21e44b4b0d9   \n",
       "1     Expert Security Agency  4c004b846168151b991c9d36b84d42c8   \n",
       "2     leekie enterprises inc  b4ab50eb7e380812afbae53f03334c7b   \n",
       "3             Mangiboa Store  51250b025291d65f4dac3246c2d961bf   \n",
       "4              Fruits Vendor  68a04ab84f0a560ea880f44d1e130008   \n",
       "\n",
       "  standardized_name_ngram standardized_name_aggressive  \n",
       "0         EEI Corporation                          eei  \n",
       "1  Expert Security Agency       agency expert security  \n",
       "2  leekie enterprises inc           enterprises leekie  \n",
       "3          Mangiboa Store               mangiboa store  \n",
       "4           Fruits Vendor                fruits vendor  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.to_csv(\"Final_cleaned_company_names_pipeline.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>global_mean</th>\n",
       "      <th>smoothing_factor</th>\n",
       "      <th>loan_company_name</th>\n",
       "      <th>freq_encodedcompanyName</th>\n",
       "      <th>category_mean_companyName</th>\n",
       "      <th>category_size_companyName</th>\n",
       "      <th>target_encodedcompanyName</th>\n",
       "      <th>encoded_company_name_group</th>\n",
       "      <th>clean_name</th>\n",
       "      <th>phonetic_code</th>\n",
       "      <th>standardized_name_phonetic</th>\n",
       "      <th>ngram_hash</th>\n",
       "      <th>standardized_name_ngram</th>\n",
       "      <th>standardized_name_aggressive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>30892.506245</td>\n",
       "      <td>15</td>\n",
       "      <td>Rajoma gifts n homes accents. Inc</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29586.724605</td>\n",
       "      <td>29586.724605</td>\n",
       "      <td>rajomagiftsnhomesaccents</td>\n",
       "      <td>RJMJFTSNHMSKSNTS</td>\n",
       "      <td>Rajoma gifts n homes accents. Inc</td>\n",
       "      <td>e50b6c4e598d11bc183eaf2884f978f5</td>\n",
       "      <td>Rajoma gifts n homes accents. Inc</td>\n",
       "      <td>accents gifts homes n rajoma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3558</th>\n",
       "      <td>30892.506245</td>\n",
       "      <td>15</td>\n",
       "      <td>accenture inc</td>\n",
       "      <td>3.0</td>\n",
       "      <td>41666.666667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>32688.199648</td>\n",
       "      <td>98064.598945</td>\n",
       "      <td>accenture</td>\n",
       "      <td>AKSNTR</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>e96a479c49f59f0f9e66875d0d856ab6</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>accenture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58328</th>\n",
       "      <td>30892.506245</td>\n",
       "      <td>15</td>\n",
       "      <td>accenture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>25333.333333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29965.977426</td>\n",
       "      <td>89897.932279</td>\n",
       "      <td>accenture</td>\n",
       "      <td>AKSNTR</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>e96a479c49f59f0f9e66875d0d856ab6</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>accenture</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        global_mean  smoothing_factor                  loan_company_name  \\\n",
       "408    30892.506245                15  Rajoma gifts n homes accents. Inc   \n",
       "3558   30892.506245                15                      accenture inc   \n",
       "58328  30892.506245                15                          accenture   \n",
       "\n",
       "       freq_encodedcompanyName  category_mean_companyName  \\\n",
       "408                        1.0               10000.000000   \n",
       "3558                       3.0               41666.666667   \n",
       "58328                      3.0               25333.333333   \n",
       "\n",
       "       category_size_companyName  target_encodedcompanyName  \\\n",
       "408                          1.0               29586.724605   \n",
       "3558                         3.0               32688.199648   \n",
       "58328                        3.0               29965.977426   \n",
       "\n",
       "       encoded_company_name_group                clean_name     phonetic_code  \\\n",
       "408                  29586.724605  rajomagiftsnhomesaccents  RJMJFTSNHMSKSNTS   \n",
       "3558                 98064.598945                 accenture            AKSNTR   \n",
       "58328                89897.932279                 accenture            AKSNTR   \n",
       "\n",
       "              standardized_name_phonetic                        ngram_hash  \\\n",
       "408    Rajoma gifts n homes accents. Inc  e50b6c4e598d11bc183eaf2884f978f5   \n",
       "3558                           Accenture  e96a479c49f59f0f9e66875d0d856ab6   \n",
       "58328                          Accenture  e96a479c49f59f0f9e66875d0d856ab6   \n",
       "\n",
       "                 standardized_name_ngram  standardized_name_aggressive  \n",
       "408    Rajoma gifts n homes accents. Inc  accents gifts homes n rajoma  \n",
       "3558                           Accenture                     accenture  \n",
       "58328                          Accenture                     accenture  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned[df_cleaned['loan_company_name'].str.contains('accent', na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
